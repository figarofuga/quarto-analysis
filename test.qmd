---
title: "Causal inference"
format: html
---

# Introduction

This is the first Quarto document of causal inference using machine learning such as Meta learner.

## Methods

The lalonde dataset is simultaneously RCT and observational study.
We hypothesized that ATT in observational study is nearest ATE in RCT. 
The estimate value in RCT is 800-900 dollars.

by "https://causalinf.substack.com/p/lalonde-40-years-later-imbens-and"

### import packages


### read lalonde dataset

```{r}
library(MatchIt)
lalonde_r <- MatchIt::lalonde

```

## regression model

```{r}
library(parameters)
library(rms)

dd <- datadist(lalonde_r)
options(datadist = "dd")

fit_ols <- rms::ols(re78 ~ treat + age + educ + race + married + nodegree + re74 + re75, data = lalonde_r)

fit_ols_rcs <- rms::ols(re78 ~ treat + rcs(age, 5) + educ + race + married + nodegree + rcs(re74, 5) + rcs(re75, 5), data = lalonde_r)

parameters::compare_parameters(fit_ols, fit_ols_rcs, keep = "treat")

```

## Propensity score matching and weighting

```{r}
library(parameters)
library(highs)
library(MatchIt)
library(rootSolve)
library(WeightIt)
library(cobalt)
library(marginaleffects)
library(broom)

mout <- MatchIt::matchit(
    treat ~ age + educ + race + married + nodegree + re74 + re75, 
    data = lalonde_r, 
    method = "cardinality", 
    distance = "glm", 
    estimand = "ATT"
    )

cobalt::bal.tab(mout)

mdata <- MatchIt::match.data(mout)

fit_match_pre <- lm(re78 ~ treat * (age + educ + race + married +
                            nodegree + re74 + re75),
          data = mdata,
          weights = weights)


fit_match <- avg_comparisons(fit_match_pre,
                variables = "treat",
                vcov = ~subclass)


wout <- WeightIt::weightit(
    treat ~ age + educ + race + married + nodegree + re74 + re75, 
    data = lalonde_r, 
    method = "ipt",
    estimand = "ATT"
)                

cobalt::bal.tab(wout)


fit_weight_pre <- lm_weightit(re78 ~ treat * (age + educ + race + married +
                                     nodegree + re74 + re75),
                   data = lalonde_r, weightit = wout)

fit_weight <- avg_comparisons(fit_weight_pre,
                variables = "treat")           


modelbased::estimate_contrasts(
    fit_weight_pre, 
    contrast = "treat", 
    estimate = "population"
)      

modelsummary::modelsummary(
    list("PS match" = fit_match, 
    "PS weight" = fit_weight)
)

```

```{r}
library(dplyr)
library(grf)

X <- model.matrix(~ age + educ + race + married + nodegree + re74 + re75 + 0, data = dplyr::select(lalonde_r, -re78, -treat))
Y <- lalonde_r$re78
W <- lalonde_r$treat

fit_grf <- grf::causal_forest(X = X, Y = Y, W = W)

grf::average_treatment_effect(fit_grf, target.sample = "overlap")

```

```{r}
library(dplyr)
library(tmle)

W <- model.matrix(~ age + educ + race + married + nodegree + re74 + re75 + 0, data = dplyr::select(lalonde_r, -re78, -treat))

Y <- lalonde_r$re78
A <- lalonde_r$treat

tmle_fit <- tmle::tmle(Y = Y, A = A, W = W, 
    family = 'gaussian', 
    Q.SL.library=c("SL.mean","SL.glm"),
    g.SL.library=c("SL.mean","SL.glm"),
    Qform = "Y~A+age+educ+raceblack+racehispan+racewhite+married+nodegree+re74+re75", 
    gform = "A~age+educ+raceblack+racewhite+racehispan+married+nodegree+re74+re75", 
    cvQinit = TRUE)

summary(tmle_fit)

```

```{r}
library(dplyr)
library(AIPW)
library(easystats)

AIPW_SL <- AIPW$new(Y= lalonde_r$re78,
                    A= lalonde_r$treat,
                    W= subset(lalonde_r,select=c("age", "educ", "race", "married", "nodegree", "re74", "re75")), 
                    Q.SL.library = c("SL.glm"),
                    g.SL.library = c("SL.glm"),
                    k_split = 5,
                    verbose=TRUE)
                
  suppressWarnings({
  AIPW_SL$fit()$summary(g.bound = 0.025)
})


  suppressWarnings({
  AIPW_SL$stratified_fit()$summary(g.bound = 0.025)
})


tmle_fit <- tmle::tmle(Y = Y, A = A, W = W, 
    family = 'gaussian', 
    Q.SL.library=c("SL.mean","SL.glm"),
    g.SL.library=c("SL.mean","SL.glm"),
    Qform = "Y~A+age+educ+raceblack+racehispan+racewhite+married+nodegree+re74+re75", 
    gform = "A~age+educ+raceblack+racewhite+racehispan+married+nodegree+re74+re75", 
    cvQinit = TRUE)


cat("\nEstimates from TMLE\n")
unlist(tmle_fit$estimates$ATE)
unlist(tmle_fit$estimates$RR)
unlist(tmle_fit$estimates$OR)

cat("\nEstimates from AIPW\n")

a_tmle <- AIPW_tmle$
  new(
    Y= lalonde_r$re78,
    A= lalonde_r$treat,
    tmle_fit = tmle_fit, 
    verbose = TRUE)$
  summary(g.bound=0.025)

```



```{r}

library(reticulate)

# reticulate::use_python(file.path(getwd(), ".venv", "bin", "python"))

```

```{python}

import pandas as pd
import polars as pl
import marginaleffects

lalonde = pd.read_csv("rawdata/lalonde.csv")
```


```{python}
import matplotlib.pyplot as plt
import seaborn as sns

lalonde_py = r.lalonde_r

lalonde_py.describe()

sns.scatterplot(data = lalonde_py, x = 're74', y = 're78', hue = 'treat')

plt.show()

```

```{python}
import statsmodels.formula.api as smf

formu = 're78 ~ treat + age + educ + race + married + nodegree + re74 + re75'

ols_fit = smf.ols(formula=formu, data = lalonde_py).fit()

ols_fit.summary()
```

```{python}
import pandas as pd
import dowhy
import networkx as nx
from dowhy import CausalModel

lalonde_post = (pd.get_dummies(lalonde, 
                         columns=["race"], dtype=int)          
)

# 2. NetworkXでグラフを定義
causal_graph = nx.DiGraph()
# "age", "educ", "married", "nodegree", "re74", "re75", "race_black", "race_hispan", "race_white"
# ノードとエッジを追加 (交絡構造: Age -> Exercise, Age -> Health)
causal_graph.add_nodes_from(["treat", "age", "educ", "married", "nodegree", "re74", "re75", "race_black", "race_hispan", "race_white", "re78"])

causal_graph.add_edges_from([
    ('treat', 're78'),
    ('re74', 're75'),
    ('age', 'treat'), 
    ('age', 're78'),
    ('educ', 'treat'),
    ('educ', 're78'),
    ('married', 'treat'),
    ('married', 're78'),
    ('nodegree', 'treat'),
    ('nodegree', 're78'),
    ('re74', 'treat'),
    ('re75', 'treat'), 
    ('re75', 're78'), 
    ('race_black', 'treat'),
    ('race_black', 're74'),
    ('race_black', 're75'),
    ('race_black', 're78'),
    ('race_hispan', 'treat'),
    ('race_hispan', 're74'),
    ('race_hispan', 're75'),
    ('race_hispan', 're78'),
    ('race_white', 'treat'),
    ('race_white', 're74'),
    ('race_white', 're75'),
    ('race_white', 're78')
    
])

# 3. NetworkXオブジェクトをGML文字列に変換
# 注意: 文字列内の改行コードなどを整形して渡します
gml_string = "".join(nx.generate_gml(causal_graph))

# 4. モデルの定義
model = CausalModel(
    data=lalonde_post,
    treatment='treat',
    outcome='re78',
    graph=gml_string
)

# 5. グラフの確認
model.view_model()

model.summary()

identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)

print(identified_estimand)

causal_estimate = model.estimate_effect(identified_estimand,
        method_name="backdoor.propensity_score_stratification")
        
print(causal_estimate)

res_random=model.refute_estimate(identified_estimand, causal_estimate, method_name="random_common_cause", show_progress_bar=True)

print(res_random)
```

### read data

```{python}
import pandas as pd
from econml.dml import LinearDML

lalonde = pd.read_csv("rawdata/lalonde.csv")

est = LinearDML()

X = lalonde.loc[:,["age", "married", "educ", "nodegree", "re74", "re75"]].to_numpy()

est.fit(lalonde['re78'], 
        lalonde['treat'], 
        X=X
        )
point = est.effect(X, T0=0, T1=1)

point = est.const_marginal_effect(X)
lb, ub = est.const_marginal_effect_interval(X, alpha=0.05)

```

```{python}

# Main imports
from econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner

# Helper imports
import numpy as np
import pandas as pd
from numpy.random import binomial, multivariate_normal, normal, uniform
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split

lalonde = pd.read_csv("rawdata/lalonde.csv")

lalonde_post = (pd.get_dummies(lalonde, 
                         columns=["race"], dtype=int)          
)

X = lalonde_post.loc[:,["age", "married", "educ", "nodegree", "re74", "re75", "race_black", "race_hispan", "race_white"]]
y = lalonde_post['re78']
T = lalonde_post['treat']
n = lalonde_post.shape[0]

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size=0.2, 
    random_state=42, 
    stratify=T
    )

T_train = T.loc[y_train.index]
T_test = T.loc[y_test.index]

# Instantiate T learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
T_learner = TLearner(models=models)
# Train T_learner
T_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
T_te = T_learner.effect(X_test)

# Instantiate S learner
overall_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(491/100))
S_learner = SLearner(overall_model=overall_model)
# Train S_learner
S_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
S_te = S_learner.effect(X_test)

# Instantiate X learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))
X_learner = XLearner(models=models, propensity_model=propensity_model)
# Train X_learner
X_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
X_te = X_learner.effect(X_test)

# Instantiate Domain Adaptation learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
final_models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))
DA_learner = DomainAdaptationLearner(models=models,
                                     final_models=final_models,
                                     propensity_model=propensity_model)
# Train DA_learner
DA_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
DA_te = DA_learner.effect(X_test)

# Instantiate Doubly Robust Learner
from econml.dr import DRLearner
outcome_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
pseudo_treatment_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))

DR_learner = DRLearner(model_regression=outcome_model, model_propensity=propensity_model,
                       model_final=pseudo_treatment_model, cv=5)
# Train DR_learner
DR_learner.fit(y, T, X=X)
# Estimate treatment effects on test data
DR_te = DR_learner.effect(X_test)

```

```{python}

### Comparison plot of the different learners
import matplotlib.pyplot as plt

X_test_num = X_test.to_numpy()

plt.figure(figsize=(7, 5))
plt.scatter(X_test_num[:, 2], T_te, label="T-learner")
plt.scatter(X_test_num[:, 2], S_te, label="S-learner")
plt.scatter(X_test_num[:, 2], DA_te, label="DA-learner")
plt.scatter(X_test_num[:, 2], X_te, label="X-learner")
plt.scatter(X_test_num[:, 2], DR_te, label="DR-learner")
plt.xlabel('education degree')
plt.ylabel('Treatment Effect')
plt.legend()
plt.show()
```