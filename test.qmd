---
title: "Habits"
author: "John Doe"
format: html
---

# Introduction

This is the first Quarto document of causal inference using machine learning such as Meta learner.

## Methods

The lalonde dataset is simultaneously RCT and observational study.
We hypothesized that ATT in observational study is nearest ATE in RCT. 
The estimate value in RCT is 800-900 dollars.

by "https://causalinf.substack.com/p/lalonde-40-years-later-imbens-and"

### import packages


### read lalonde dataset


```{r}
library(MatchIt)
lalonde_r <- MatchIt::lalonde

```

## regression model

```{r}
library(parameters)
library(rms)

dd <- datadist(lalonde_r)
options(datadist = "dd")

fit_ols <- rms::ols(re78 ~ treat + age + educ + race + married + nodegree + re74 + re75, data = lalonde_r)

fit_ols_rcs <- rms::ols(re78 ~ treat + rcs(age, 5) + educ + race + married + nodegree + rcs(re74, 5) + rcs(re75, 5), data = lalonde_r)

parameters::compare_parameters(fit_ols, fit_ols_rcs, keep = "treat")

```

## Propensity score matching and weighting

```{r}
library(parameters)
library(highs)
library(MatchIt)
library(rootSolve)
library(WeightIt)
library(cobalt)
library(marginaleffects)
library(broom)

mout <- MatchIt::matchit(
    treat ~ age + educ + race + married + nodegree + re74 + re75, 
    data = lalonde_r, 
    method = "cardinality", 
    distance = "glm", 
    estimand = "ATT"
    )

cobalt::bal.tab(mout)

mdata <- MatchIt::match.data(mout)

fit_match_pre <- lm(re78 ~ treat * (age + educ + race + married +
                            nodegree + re74 + re75),
          data = mdata,
          weights = weights)


fit_match <- avg_comparisons(fit_match_pre,
                variables = "treat",
                vcov = ~subclass)


wout <- WeightIt::weightit(
    treat ~ age + educ + race + married + nodegree + re74 + re75, 
    data = lalonde_r, 
    method = "ipt",
    estimand = "ATT"
)                

cobalt::bal.tab(wout)


fit_weight_pre <- lm_weightit(re78 ~ treat * (age + educ + race + married +
                                     nodegree + re74 + re75),
                   data = lalonde_r, weightit = wout)

fit_weight <- avg_comparisons(fit_weight_pre,
                variables = "treat")           


modelbased::estimate_contrasts(
    fit_weight_pre, 
    contrast = "treat", 
    estimate = "population"
)      

modelsummary::modelsummary(
    list("PS match" = fit_match, 
    "PS weight" = fit_weight)
)

```

```{r}
library(dplyr)
library(grf)

X <- model.matrix(~ age + educ + race + married + nodegree + re74 + re75 + 0, data = dplyr::select(lalonde_r, -re78, -treat))
Y <- lalonde_r$re78
W <- lalonde_r$treat

fit_grf <- grf::causal_forest(X = X, Y = Y, W = W)

grf::average_treatment_effect(fit_grf, target.sample = "overlap")

```

```{r}
library(dplyr)
library(tmle)

# --- 0) 準備 ---------------------------------------------------------------
library(MatchIt)
data("lalonde")

dat <- lalonde |> 
    tidyr::drop_na()

# 処置・アウトカム・共変量
A <- dat$treat
Y <- dat$re78

W <- dat[, c("age","educ","race","married","nodegree","re74","re75")]

# 欠損があれば落とす（lalonde は通常欠損なし想定）

n <- length(Y)

# --- 1) まずは素朴な推定（比較用） ----------------------------------------
naive_diff <- mean(Y[A==1]) - mean(Y[A==0])

# --- 2) gモデル（傾向スコア）: P(A=1|W) -----------------------------------
g_fit <- glm(treat ~ age + educ + race + married + nodegree + re74 + re75, data = dat, family = binomial())
g_hat <- predict(g_fit, type = "response")

# 実務では positivity の極端さ対策で truncation をよく入れる
g_hat <- pmin(pmax(g_hat, 0.01), 0.99)

# --- 3) Qモデル（アウトカム回帰）: E(Y|A,W) ---------------------------------
Q_fit <- glm(re78 ~ ., data = dat, family = gaussian())

# 観測された A の下での予測 Q(A,W)
Q_aw <- predict(Q_fit, type = "response")

# 介入した世界（A=1, A=0）の下での予測 Q(1,W), Q(0,W)

Q1_w <- predict(Q_fit, newdata = dplyr::mutate(dat, treat = 1), type = "response")
Q0_w <- predict(Q_fit, newdata = dplyr::mutate(dat, treat = 0), type = "response")

# ここまでが「G-computation（回帰による反実仮想平均）」の素材
gcomp_ate <- mean(Q1_w - Q0_w)

# --- 4) clever covariate（IPWっぽい重み） -----------------------------------
# 連続アウトカムの基本形では、H1, H0 を使って Q を "必要な方向に" だけ更新する
H1 <- as.numeric(dat$treat == 1) / g_hat
H0 <- as.numeric(dat$treat == 0) / (1 - g_hat)

# --- 5) targeting step：epsilon を推定して Q を更新 --------------------------
# 連続(Y)の簡単な形： (Y - Q_aw) を H1, H0 で回帰（切片なし）して epsilon を得る

dat2 <- dplyr::mutate(dat, 
    Q_aw = Q_aw, 
    H1 = H1, 
    H0 = H0)

eps_fit <- lm(I(re78 - Q_aw) ~ -1 + H1 + H0, data = dat2)
eps <- coef(eps_fit)
eps1 <- unname(eps["H1"])
eps0 <- unname(eps["H0"])

# 更新後の Q*（observed と intervention の両方）
Qstar_aw <- Q_aw + eps1 * H1 + eps0 * H0
Qstar_1w <- Q1_w + eps1 * (1 / g_hat)
Qstar_0w <- Q0_w + eps0 * (1 / (1 - g_hat))

tmle_ate <- mean(Qstar_1w - Qstar_0w)

# --- 6) 標準誤差（influence function の分散で） ----------------------------
# IC = H1*(Y - Q*) - H0*(Y - Q*) + (Q*(1,W)-Q*(0,W)) - psi
IC <- (H1 - H0) * (Y - Qstar_aw) + (Qstar_1w - Qstar_0w) - tmle_ate
se <- sd(IC) / sqrt(n)
ci <- tmle_ate + c(-1, 1) * 1.96 * se

# --- 7) 結果表示 -----------------------------------------------------------
cat("n =", n, "\n")
cat("Naive diff (unadjusted):", naive_diff, "\n")
cat("G-computation ATE (Q-model only):", gcomp_ate, "\n")
cat("TMLE ATE (targeted):", tmle_ate, "\n")
cat("TMLE 95% CI:", ci[1], "to", ci[2], "\n")


library(tmle)

W <- model.matrix(~ age + educ + race + married + nodegree + re74 + re75 + 0, data = dplyr::select(dat, -re78, -treat))

Y <- dat$re78
A <- dat$treat

tmle_fit <- tmle::tmle(Y = Y, A = A, W = W, 
    family = 'gaussian', 
    Q.SL.library=c("SL.glm"),
    g.SL.library=c("SL.glm"),
    Qform = "Y~A+age+educ+raceblack+racehispan+racewhite+married+nodegree+re74+re75", 
    gform = "A~age+educ+raceblack+racewhite+racehispan+married+nodegree+re74+re75", 
    cvQinit = TRUE)

summary(tmle_fit)

tmle_fit$estimates$ATE$psi
tmle_fit$estimates$ATE$CI
tmle_fit$estimates$ATE$var.psi


```

```{r}
library(dplyr)
library(AIPW)
library(easystats)

AIPW_SL <- AIPW$new(Y= lalonde_r$re78,
                    A= lalonde_r$treat,
                    W= subset(lalonde_r,select=c("age", "educ", "race", "married", "nodegree", "re74", "re75")), 
                    Q.SL.library = c("SL.glm"),
                    g.SL.library = c("SL.glm"),
                    k_split = 5,
                    verbose=TRUE)
                
  suppressWarnings({
  AIPW_SL$fit()$summary(g.bound = 0.025)
})


  suppressWarnings({
  AIPW_SL$stratified_fit()$summary(g.bound = 0.025)
})


tmle_fit <- tmle::tmle(Y = Y, A = A, W = W, 
    family = 'gaussian', 
    Q.SL.library=c("SL.mean","SL.glm"),
    g.SL.library=c("SL.mean","SL.glm"),
    Qform = "Y~A+age+educ+raceblack+racehispan+racewhite+married+nodegree+re74+re75", 
    gform = "A~age+educ+raceblack+racewhite+racehispan+married+nodegree+re74+re75", 
    cvQinit = TRUE)


cat("\nEstimates from TMLE\n")
unlist(tmle_fit$estimates$ATE)
unlist(tmle_fit$estimates$RR)
unlist(tmle_fit$estimates$OR)

cat("\nEstimates from AIPW\n")

a_tmle <- AIPW_tmle$
  new(
    Y= lalonde_r$re78,
    A= lalonde_r$treat,
    tmle_fit = tmle_fit, 
    verbose = TRUE)$
  summary(g.bound=0.025)

```

```{r}

# For X learner

library(tidymodels)
library(dplyr)

set.seed(1)

# -----------------------
# Simulate data (binary treatment, continuous outcome)
# -----------------------
n <- 2000
p <- 6
X <- matrix(rnorm(n * p), n, p)
colnames(X) <- paste0("x", 1:p)

lin_p  <- 0.3 * X[,1] - 0.2 * X[,2] + 0.15 * X[,3]
e_true <- plogis(lin_p)
w      <- rbinom(n, 1, e_true)

mu0_true  <- 0.5 * X[,1] - 0.8 * X[,2] + 0.2 * X[,4]^2
tau_true  <- 1.0 + 0.7 * X[,1] - 0.4 * X[,2]
y         <- mu0_true + w * tau_true + rnorm(n, sd = 1.0)

dat <- as_tibble(X) |>
  mutate(y = y, w = w, w_f = factor(w, levels = c(0, 1)), tau_true = tau_true)

xvars <- paste0("x", 1:p)

# -----------------------
# Train/Test split
# -----------------------
set.seed(2)
sp    <- initial_split(dat, prop = 0.7, strata = w_f)
train <- training(sp)
test  <- testing(sp)

# -----------------------
# Base learners
# -----------------------
outcome_spec <- rand_forest(trees = 500, mtry = 3, min_n = 10) |>
  set_engine("ranger") |>
  set_mode("regression")

tau_spec <- boost_tree(trees = 800, mtry = 3, min_n = 10) |>
  set_engine("xgboost") |>
  set_mode("regression")

prop_spec <- logistic_reg(penalty = 0.001, mixture = 1) |>
  set_engine("glmnet")

# -----------------------
# Step 1: nuisance outcome models mu0(x), mu1(x)
# -----------------------
mu0_fit <- workflow() |>
  add_recipe(recipe(y ~ ., data = train |> filter(w == 0) |> select(all_of(xvars), y))) |>
  add_model(outcome_spec) |>
  fit(data = train |> filter(w == 0) |> select(all_of(xvars), y))

mu1_fit <- workflow() |>
  add_recipe(recipe(y ~ ., data = train |> filter(w == 1) |> select(all_of(xvars), y))) |>
  add_model(outcome_spec) |>
  fit(data = train |> filter(w == 1) |> select(all_of(xvars), y))

# train上での予測（imputed effectsを作るため）
mu0_hat_tr <- predict(mu0_fit, new_data = train |> select(all_of(xvars)))$.pred
mu1_hat_tr <- predict(mu1_fit, new_data = train |> select(all_of(xvars)))$.pred

train2 <- train |>
  mutate(mu0_hat = mu0_hat_tr, mu1_hat = mu1_hat_tr) |>
  mutate(
    d1 = if_else(w == 1, y - mu0_hat, NA_real_),   # treated: Y - mu0(x)
    d0 = if_else(w == 0, mu1_hat - y, NA_real_)    # control: mu1(x) - Y
  )

# -----------------------
# Step 2: fit tau1(x)=E[d1|X,w=1], tau0(x)=E[d0|X,w=0]
# -----------------------
tau1_fit <- workflow() |>
  add_recipe(recipe(d1 ~ ., data = train2 |> filter(w == 1) |> select(all_of(xvars), d1))) |>
  add_model(tau_spec) |>
  fit(data = train2 |> filter(w == 1) |> select(all_of(xvars), d1))

tau0_fit <- workflow() |>
  add_recipe(recipe(d0 ~ ., data = train2 |> filter(w == 0) |> select(all_of(xvars), d0))) |>
  add_model(tau_spec) |>
  fit(data = train2 |> filter(w == 0) |> select(all_of(xvars), d0))

# -----------------------
# Step 3: propensity e(x) で重み g(x)=e(x) を作る
# -----------------------
e_fit <- workflow() |>
  add_recipe(recipe(w_f ~ ., data = train |> select(all_of(xvars), w_f))) |>
  add_model(prop_spec) |>
  fit(data = train |> select(all_of(xvars), w_f))

e_hat_te <- predict(e_fit, new_data = test |> select(all_of(xvars)), type = "prob")$.pred_1
e_hat_te <- pmin(pmax(e_hat_te, 0.01), 0.99)

# -----------------------
# Predict on test & combine
#   tau_X(x) = e(x)*tau0(x) + (1-e(x))*tau1(x)
# -----------------------
tau1_hat_te <- predict(tau1_fit, new_data = test |> select(all_of(xvars)))$.pred
tau0_hat_te <- predict(tau0_fit, new_data = test |> select(all_of(xvars)))$.pred

tau_x <- e_hat_te * tau0_hat_te + (1 - e_hat_te) * tau1_hat_te

rmse <- function(a, b) sqrt(mean((a - b)^2))
cat("RMSE (X-learner):", rmse(tau_x, test$tau_true), "\n")

bind_cols(test |> select(tau_true), tibble(tau_x = tau_x, e_hat = e_hat_te)) |>
  slice_head(n = 10)

```

```{r}
# DR learner for R 
library(tidymodels)
library(dplyr)

set.seed(1)

# -----------------------
# Simulate data (binary treatment, continuous outcome)
# -----------------------
n <- 2000
p <- 6
X <- matrix(rnorm(n * p), n, p)
colnames(X) <- paste0("x", 1:p)

lin_p  <- 0.3 * X[,1] - 0.2 * X[,2] + 0.15 * X[,3]
e_true <- plogis(lin_p)
w      <- rbinom(n, 1, e_true)

mu0_true  <- 0.5 * X[,1] - 0.8 * X[,2] + 0.2 * X[,4]^2
tau_true  <- 1.0 + 0.7 * X[,1] - 0.4 * X[,2]
y         <- mu0_true + w * tau_true + rnorm(n, sd = 1.0)

dat <- as_tibble(X) |>
  mutate(y = y, w = w, w_f = factor(w, levels = c(0, 1)), tau_true = tau_true)

xvars <- paste0("x", 1:p)

# -----------------------
# Train/Test split
# -----------------------
set.seed(2)
sp    <- initial_split(dat, prop = 0.7, strata = w_f)
train <- training(sp)
test  <- testing(sp)

# -----------------------
# Base learners
# -----------------------
outcome_spec <- rand_forest(trees = 500, mtry = 3, min_n = 10) |>
  set_engine("ranger") |>
  set_mode("regression")

tau_spec <- rand_forest(trees = 800, mtry = 3, min_n = 10) |>
  set_engine("ranger") |>
  set_mode("regression")

prop_spec <- logistic_reg(penalty = 0.001, mixture = 1) |>
  set_engine("glmnet")

# -----------------------
# Step 1: nuisance mu0(x), mu1(x), e(x)
# -----------------------
mu0_fit <- workflow() |>
  add_recipe(recipe(y ~ ., data = train |> filter(w == 0) |> select(all_of(xvars), y))) |>
  add_model(outcome_spec) |>
  fit(data = train |> filter(w == 0) |> select(all_of(xvars), y))

mu1_fit <- workflow() |>
  add_recipe(recipe(y ~ ., data = train |> filter(w == 1) |> select(all_of(xvars), y))) |>
  add_model(outcome_spec) |>
  fit(data = train |> filter(w == 1) |> select(all_of(xvars), y))

e_fit <- workflow() |>
  add_recipe(recipe(w_f ~ ., data = train |> select(all_of(xvars), w_f))) |>
  add_model(prop_spec) |>
  fit(data = train |> select(all_of(xvars), w_f))

# train上の nuisance 予測
mu0_hat_tr <- predict(mu0_fit, new_data = train |> select(all_of(xvars)))$.pred
mu1_hat_tr <- predict(mu1_fit, new_data = train |> select(all_of(xvars)))$.pred
e_hat_tr   <- predict(e_fit,   new_data = train |> select(all_of(xvars)), type = "prob")$.pred_1
e_hat_tr   <- pmin(pmax(e_hat_tr, 0.01), 0.99)

mu_w_hat_tr <- if_else(train$w == 1, mu1_hat_tr, mu0_hat_tr)

# -----------------------
# Step 2: DR pseudo-outcome φ (Kennedy 2020)
#   φ = (w-e)/(e(1-e)) * (y - μ_w) + (μ1 - μ0)
# -----------------------
phi_tr <- (train$w - e_hat_tr) / (e_hat_tr * (1 - e_hat_tr)) * (train$y - mu_w_hat_tr) +
  (mu1_hat_tr - mu0_hat_tr)

train2 <- train |>
  mutate(phi = phi_tr)

# -----------------------
# Step 3: fit tau(x) = E[phi | X=x]
# -----------------------
tau_dr_fit <- workflow() |>
  add_recipe(recipe(phi ~ ., data = train2 |> select(all_of(xvars), phi))) |>
  add_model(tau_spec) |>
  fit(data = train2 |> select(all_of(xvars), phi))

# -----------------------
# Predict on test
# -----------------------
tau_dr <- predict(tau_dr_fit, new_data = test |> select(all_of(xvars)))$.pred

rmse <- function(a, b) sqrt(mean((a - b)^2))
cat("RMSE (DR-learner):", rmse(tau_dr, test$tau_true), "\n")

bind_cols(test |> select(tau_true), tibble(tau_dr = tau_dr)) |>
  slice_head(n = 10)

```

```{r}

library(reticulate)

# reticulate::use_python(file.path(getwd(), ".venv", "bin", "python"))

```

```{python}

import pandas as pd
import polars as pl
import marginaleffects

lalonde = pd.read_csv("rawdata/lalonde.csv")
```


```{python}
import matplotlib.pyplot as plt
import seaborn as sns

lalonde_py = r.lalonde_r

lalonde_py.describe()

sns.scatterplot(data = lalonde_py, x = 're74', y = 're78', hue = 'treat')

plt.show()

```

```{python}
import statsmodels.formula.api as smf

formu = 're78 ~ treat + age + educ + race + married + nodegree + re74 + re75'

ols_fit = smf.ols(formula=formu, data = lalonde_py).fit()

ols_fit.summary()
```

```{python}
import pandas as pd
import dowhy
import networkx as nx
from dowhy import CausalModel

lalonde_post = (pd.get_dummies(lalonde, 
                         columns=["race"], dtype=int)          
)

# 2. NetworkXでグラフを定義
causal_graph = nx.DiGraph()
# "age", "educ", "married", "nodegree", "re74", "re75", "race_black", "race_hispan", "race_white"
# ノードとエッジを追加 (交絡構造: Age -> Exercise, Age -> Health)
causal_graph.add_nodes_from(["treat", "age", "educ", "married", "nodegree", "re74", "re75", "race_black", "race_hispan", "race_white", "re78"])

causal_graph.add_edges_from([
    ('treat', 're78'),
    ('re74', 're75'),
    ('age', 'treat'), 
    ('age', 're78'),
    ('educ', 'treat'),
    ('educ', 're78'),
    ('married', 'treat'),
    ('married', 're78'),
    ('nodegree', 'treat'),
    ('nodegree', 're78'),
    ('re74', 'treat'),
    ('re75', 'treat'), 
    ('re75', 're78'), 
    ('race_black', 'treat'),
    ('race_black', 're74'),
    ('race_black', 're75'),
    ('race_black', 're78'),
    ('race_hispan', 'treat'),
    ('race_hispan', 're74'),
    ('race_hispan', 're75'),
    ('race_hispan', 're78'),
    ('race_white', 'treat'),
    ('race_white', 're74'),
    ('race_white', 're75'),
    ('race_white', 're78')
    
])

# 3. NetworkXオブジェクトをGML文字列に変換
# 注意: 文字列内の改行コードなどを整形して渡します
gml_string = "".join(nx.generate_gml(causal_graph))

# 4. モデルの定義
model = CausalModel(
    data=lalonde_post,
    treatment='treat',
    outcome='re78',
    graph=gml_string
)

# 5. グラフの確認
model.view_model()

model.summary()

identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)

print(identified_estimand)

causal_estimate = model.estimate_effect(identified_estimand,
        method_name="backdoor.propensity_score_stratification")
        
print(causal_estimate)

res_random=model.refute_estimate(identified_estimand, causal_estimate, method_name="random_common_cause", show_progress_bar=True)

print(res_random)
```

### read data

```{python}
import pandas as pd
from econml.dml import LinearDML

lalonde = pd.read_csv("rawdata/lalonde.csv")

est = LinearDML()

X = lalonde.loc[:,["age", "married", "educ", "nodegree", "re74", "re75"]].to_numpy()

est.fit(lalonde['re78'], 
        lalonde['treat'], 
        X=X
        )
point = est.effect(X, T0=0, T1=1)

point = est.const_marginal_effect(X)
lb, ub = est.const_marginal_effect_interval(X, alpha=0.05)

```

```{python}

# Main imports
from econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner

# Helper imports
import numpy as np
import pandas as pd
from numpy.random import binomial, multivariate_normal, normal, uniform
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split

lalonde = pd.read_csv("rawdata/lalonde.csv")

lalonde_post = (pd.get_dummies(lalonde, 
                         columns=["race"], dtype=int)          
)

X = lalonde_post.loc[:,["age", "married", "educ", "nodegree", "re74", "re75", "race_black", "race_hispan", "race_white"]]
y = lalonde_post['re78']
T = lalonde_post['treat']
n = lalonde_post.shape[0]

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size=0.2, 
    random_state=42, 
    stratify=T
    )

T_train = T.loc[y_train.index]
T_test = T.loc[y_test.index]

# Instantiate T learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
T_learner = TLearner(models=models)
# Train T_learner
T_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
T_te = T_learner.effect(X_test)

# Instantiate S learner
overall_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(491/100))
S_learner = SLearner(overall_model=overall_model)
# Train S_learner
S_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
S_te = S_learner.effect(X_test)

# Instantiate X learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))
X_learner = XLearner(models=models, propensity_model=propensity_model)
# Train X_learner
X_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
X_te = X_learner.effect(X_test)

# Instantiate Domain Adaptation learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
final_models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))
DA_learner = DomainAdaptationLearner(models=models,
                                     final_models=final_models,
                                     propensity_model=propensity_model)
# Train DA_learner
DA_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
DA_te = DA_learner.effect(X_test)

# Instantiate Doubly Robust Learner
from econml.dr import DRLearner
outcome_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
pseudo_treatment_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))

DR_learner = DRLearner(model_regression=outcome_model, model_propensity=propensity_model,
                       model_final=pseudo_treatment_model, cv=5)
# Train DR_learner
DR_learner.fit(y, T, X=X)
# Estimate treatment effects on test data
DR_te = DR_learner.effect(X_test)

```

```{python}

### Comparison plot of the different learners
import matplotlib.pyplot as plt

X_test_num = X_test.to_numpy()

plt.figure(figsize=(7, 5))
plt.scatter(X_test_num[:, 2], T_te, label="T-learner")
plt.scatter(X_test_num[:, 2], S_te, label="S-learner")
plt.scatter(X_test_num[:, 2], DA_te, label="DA-learner")
plt.scatter(X_test_num[:, 2], X_te, label="X-learner")
plt.scatter(X_test_num[:, 2], DR_te, label="DR-learner")
plt.xlabel('education degree')
plt.ylabel('Treatment Effect')
plt.legend()
plt.show()
```