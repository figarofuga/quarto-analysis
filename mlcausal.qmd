---
title: "機械学習を用いた因果推論"
author: "Nozomi Niimi"
format: 
  clean-typst: 
    mainfont: "Noto Sans CJK JP"
    sansfont: "Noto Sans CJK JP"
date: today
date-format: long
execute:
  echo: false
  warning: false
  error: false
# jupyter: python3
---

```{python}
#| label: data-loading


import pandas as pd

lalonde = (
    pd.read_csv("rawdata/lalonde.csv")
    .pipe(pd.get_dummies, columns=["race"], drop_first=True, dtype=int)
)

```

# 因果推論 at glance

## 因果関係

- 医学研究の第一の目標と言っても過言ではない
- Aの薬を使うと患者の予後は良くなるか？
- 実際は口で言うほど簡単ではない

## 因果関係の考え方

- 大きく分けて2つの考え方がある
  - 潜在的アウトカムを考えるRubin流
  - DAGを作り、do operatorを使うPearl流
- 詳細は省くが、今回はPearl流のDAGを中心に考える

## DAGの基本

```{python}
#| label: DAG

import graphviz

dot = graphviz.Digraph(format='png')
dot.attr(rankdir="TB")  # 上→下

# ノード
dot.node('A', '曝露')
dot.node('B', '中間因子')
dot.node('C', 'アウトカム')
dot.node('D', '交絡因子')
dot.node('E', 'Collider')

# ランク指定
with dot.subgraph() as s:
    s.attr(rank='source')   # 最上段
    s.node('E')

with dot.subgraph() as s:
    s.attr(rank='same')     # 真ん中段
    s.node('A')
    s.node('B')
    s.node('C')

with dot.subgraph() as s:
    s.attr(rank='sink')     # 最下段
    s.node('D')

# 真ん中段の左右順を固定（見えないエッジ）
dot.edge('A', 'B', style='invis')
dot.edge('B', 'C', style='invis')

# D を B の少し下に寄せる（見えないエッジ）
dot.edge('B', 'D', style='invis')

# 実際の因果エッジ
# 真ん中段の矢印は楕円の右中央 -> 左中央に固定
dot.edge('A', 'B', tailport='e', headport='w')
dot.edge('B', 'C', tailport='e', headport='w')
dot.edge('D', 'A', constraint='false')  # レイアウトへの影響を弱める
dot.edge('D', 'C', constraint='false')
dot.edge('A', 'E')
dot.edge('C', 'E')

dot


```

## 通常の統計学的手法

- 通常は交絡因子の調整により因果関係を推測する


## 交絡因子の調整の方法

- 回帰
- Matching
- 層別化

## 例えば単純な回帰だと
```{python}
#| label: simple-regression
#| results: asis

import pandas as pd
import numpy as np
import statsmodels.formula.api as smf


fit_ols = smf.ols("re78 ~ treat + age + educ + race_hispan + race_white + married + nodegree + re74 + re75", data = lalonde)

res_ols = fit_ols.fit()

coef_table = (
    res_ols.summary2().tables[1]
    .rename(columns={"Coef.": "Coef", "Std.Err.": "Std.Err", "P>|t|": "P"})
    [["Coef", "Std.Err", "t", "P"]]
    .round(2)
    .reset_index()
    .rename(columns={"index": "term"})
)

print(coef_table.to_markdown(index=False, tablefmt="pipe"))


```

## Matchingだと

- 一番有名なのはPropensity score matching

```{python}
#| label: propensity-score-analysis

import numpy as np
import pandas as pd

from causalml.propensity import LogisticRegressionPropensityModel
from causalml.match import NearestNeighborMatch

# -----------------------
# 2) Estimate propensity score with Logistic Regression
# -----------------------

ps_model = LogisticRegressionPropensityModel()  
p_score = ps_model.fit_predict(lalonde[["age", "educ", "married", "nodegree", "re74", "re75", "race_hispan", "race_white"]], lalonde["treat"])

lalonde_pm = (
  lalonde
  .assign(propensity_score = p_score)
  
)

# -----------------------
# 3) Propensity score matching (Nearest Neighbor)
# -----------------------
matcher = NearestNeighborMatch(
    caliper=0.1,     # PS差がこの範囲以内のみマッチ (例) :contentReference[oaicite:2]{index=2}
    replace=False,    # 置換なし
    ratio=1,          # 1:1 matching
    random_state=42
)

matched = matcher.match(
    data=lalonde_pm,
    treatment_col="treat",
    score_cols=["propensity_score"]  # ここにPS列名
)

print(matched.head())
print("matched size:", matched.shape)

# -----------------------
# 4) Simple ATT estimate on matched sample (difference in means)
# -----------------------
att = matched.loc[matched["treat"] == 1, "re78"].mean() - matched.loc[matched["treat"] == 0, "re78"].mean()
print("ATT (diff in means on matched sample):", att)

```


## DAGつき

```{python}
#| label: examination-Dowhy


import pandas as pd
import dowhy
import networkx as nx
from dowhy import CausalModel


# 2. NetworkXでグラフを定義
causal_graph = nx.DiGraph()
# "age", "educ", "married", "nodegree", "re74", "re75", "race_black", "race_hispan", "race_white"
# ノードとエッジを追加 (交絡構造: Age -> Exercise, Age -> Health)
causal_graph.add_nodes_from(["treat", "age", "educ", "married", "nodegree", "re74", "re75", "race_black", "race_hispan", "re78"])

causal_graph.add_edges_from([
    ('treat', 're78'),
    ('re74', 're75'),
    ('age', 'treat'), 
    ('age', 're78'),
    ('educ', 'treat'),
    ('educ', 're78'),
    ('married', 'treat'),
    ('married', 're78'),
    ('nodegree', 'treat'),
    ('nodegree', 're78'),
    ('re74', 'treat'),
    ('re75', 'treat'), 
    ('re75', 're78'), 
    ('race_black', 'treat'),
    # ('race_black', 're74'),
    # ('race_black', 're75'),
    # ('race_black', 're78'),
    ('race_hispan', 'treat')
    # ('race_hispan', 're74'),
    # ('race_hispan', 're75'),
    # ('race_hispan', 're78')
    
])

# 3. NetworkXオブジェクトをGML文字列に変換
# 注意: 文字列内の改行コードなどを整形して渡します
gml_string = "".join(nx.generate_gml(causal_graph))

# 4. モデルの定義
model = CausalModel(
    data=lalonde,
    treatment='treat',
    outcome='re78',
    graph=gml_string
)

# 5. グラフの確認
model.view_model()

model.summary()

identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)

print(identified_estimand)

causal_estimate = model.estimate_effect(identified_estimand,
        method_name="backdoor.propensity_score_matching")
        
print(causal_estimate)

res_random=model.refute_estimate(identified_estimand, causal_estimate, method_name="random_common_cause", show_progress_bar=True)

print(res_random)

```

## Doubly robust estimation 

```{python}
#| label: AIPW


import numpy as np
import pandas as pd
import statsmodels.formula.api as smf

# 1) load + dummies (method chain)


Y = "re78"
T = "treat"
X = ["age", "educ", "married", "nodegree", "re74", "re75", "race_hispan", "race_white"]
rhs = " + ".join(X)

# 2) Propensity model: logistic regression
ps_fit = smf.logit("treat ~ age + educ + married + nodegree + re74 + re75 + race_hispan + race_white", data=lalonde).fit(disp=0)
e_hat = ps_fit.predict(lalonde).clip(1e-3, 1 - 1e-3)  # stabilize

# 3) Outcome models: treated / control separately
mu1_fit = smf.ols("re78 ~ treat + age + educ + married + nodegree + re74 + re75 + race_hispan + race_white", data=lalonde.query("treat == 1")).fit()
mu0_fit = smf.ols("re78 ~ treat + age + educ + married + nodegree + re74 + re75 + race_hispan + race_white", data=lalonde.query("treat == 0")).fit()

mu1_hat = mu1_fit.predict(lalonde)
mu0_hat = mu0_fit.predict(lalonde)

# 4) AIPW score (ATE)
phi = (
    (mu1_hat - mu0_hat)
    + lalonde[T] * (lalonde[Y] - mu1_hat) / e_hat
    - (1 - lalonde[T]) * (lalonde[Y] - mu0_hat) / (1 - e_hat)
)

ate_dr = phi.mean()
se_dr = phi.std(ddof=1) / np.sqrt(len(lalonde))
ci95 = (ate_dr - 1.96 * se_dr, ate_dr + 1.96 * se_dr)

print(f"DR ATE: {ate_dr:.2f}")
print(f"SE: {se_dr:.2f}")
print(f"95% CI: ({ci95[0]:.2f}, {ci95[1]:.2f})")


```

## ここらへんの問題

- Misspecificationの問題
- Estimandの問題 = 治療効果の異質性

# 機械学習 at glance



## 機械学習とは


## 代表的な機械学習

- Logistic回帰
- tree-based model
  - Random forest
  - XGBoost
- Neural network


## 機械学習の強み

## 疑問

- 機械学習を用いて因果関係を推測出来るんじゃないか？

## 機械学習を用いた因果推論のPros and Cons

- Pros
  - モデルのMisspecificationを避けられる
  - 因果関係の異質性を柔軟に捉えられる
- Cons
  - Overfittingの問題
  - 解釈が難しい時がある
  - 信頼区間が出しにくい

```{python}
#| label: meta-learner


# Main imports
from econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner

# Helper imports
import numpy as np
import pandas as pd
from numpy.random import binomial, multivariate_normal, normal, uniform
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split

X = lalonde.loc[:,["age", "married", "educ", "nodegree", "re74", "re75", "race_hispan", "race_white"]]
y = lalonde['re78']
T = lalonde['treat']
n = lalonde.shape[0]

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size=0.2, 
    random_state=42, 
    stratify=T
    )

T_train = T.loc[y_train.index]
T_test = T.loc[y_test.index]

# Instantiate T learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
T_learner = TLearner(models=models)
# Train T_learner
T_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
T_te = T_learner.effect(X_test)

# Instantiate S learner
overall_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(491/100))
S_learner = SLearner(overall_model=overall_model)
# Train S_learner
S_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
S_te = S_learner.effect(X_test)

print(S_te)

# Instantiate X learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))
X_learner = XLearner(models=models, propensity_model=propensity_model)
# Train X_learner
X_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
X_te = X_learner.effect(X_test)

# Instantiate Domain Adaptation learner
models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
final_models = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))
DA_learner = DomainAdaptationLearner(models=models,
                                     final_models=final_models,
                                     propensity_model=propensity_model)
# Train DA_learner
DA_learner.fit(y_train, T_train, X=X_train)
# Estimate treatment effects on test data
DA_te = DA_learner.effect(X_test)

# Instantiate Doubly Robust Learner
from econml.dr import DRLearner
outcome_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
pseudo_treatment_model = GradientBoostingRegressor(n_estimators=100, max_depth=6, min_samples_leaf=int(n/100))
propensity_model = RandomForestClassifier(n_estimators=100, max_depth=6,
                                                  min_samples_leaf=int(n/100))

DR_learner = DRLearner(model_regression=outcome_model, model_propensity=propensity_model,
                       model_final=pseudo_treatment_model, cv=5)
# Train DR_learner
DR_learner.fit(y, T, X=X)
# Estimate treatment effects on test data
DR_te = DR_learner.effect(X_test)

```
## 機械学習の因果推論における使い方

- 交絡因子の調整で機械学習を用いる
  - AIPW, tlmeなど
- 因果をダイレクトに推論する学習機を作る
  - Meta-learner
- CATEを測定する
  - Causal forestなど

## 因果forest

- HTEを計測するのに有名
- Ashey et al 2019


## Survival model

```{r}
#| label: survival-model

library(here)
library(readr)

lalonde <- readr::read_csv(here::here("rawdata", "lalonde.csv"))

head(lalonde)

```