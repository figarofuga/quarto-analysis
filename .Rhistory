reticulate::repl_python()
reticulate::source_python('/workspaces/quarto-analysis/test.py')
library(MatchIt)
lalonde_r <- MatchIt::lalonde
library(parameters)
library(rms)
dd <- datadist(lalonde_r)
options(datadist = "dd")
fit_ols <- rms::ols(re78 ~ treat + age + educ + race + married + nodegree + re74 + re75, data = lalonde_r)
fit_ols_rcs <- rms::ols(re78 ~ treat + rcs(age, 5) + educ + race + married + nodegree + rcs(re74, 5) + rcs(re75, 5), data = lalonde_r)
parameters::compare_parameters(fit_ols, fit_ols_rcs, keep = "treat")
library(parameters)
library(highs)
library(MatchIt)
library(rootSolve)
library(WeightIt)
library(cobalt)
library(marginaleffects)
library(broom)
mout <- MatchIt::matchit(
treat ~ age + educ + race + married + nodegree + re74 + re75,
data = lalonde_r,
method = "cardinality",
distance = "glm",
estimand = "ATT"
)
cobalt::bal.tab(mout)
mdata <- MatchIt::match.data(mout)
fit_match_pre <- lm(re78 ~ treat * (age + educ + race + married +
nodegree + re74 + re75),
data = mdata,
weights = weights)
fit_match <- avg_comparisons(fit_match_pre,
variables = "treat",
vcov = ~subclass)
wout <- WeightIt::weightit(
treat ~ age + educ + race + married + nodegree + re74 + re75,
data = lalonde_r,
method = "ipt",
estimand = "ATT"
)
cobalt::bal.tab(wout)
fit_weight_pre <- lm_weightit(re78 ~ treat * (age + educ + race + married +
nodegree + re74 + re75),
data = lalonde_r, weightit = wout)
fit_weight <- avg_comparisons(fit_weight_pre,
variables = "treat")
modelbased::estimate_contrasts(
fit_weight_pre,
contrast = "treat",
estimate = "population"
)
modelsummary::modelsummary(
list("PS match" = fit_match,
"PS weight" = fit_weight)
)
library(dplyr)
library(grf)
X <- model.matrix(~ age + educ + race + married + nodegree + re74 + re75 + 0, data = dplyr::select(lalonde_r, -re78, -treat))
Y <- lalonde_r$re78
W <- lalonde_r$treat
fit_grf <- grf::causal_forest(X = X, Y = Y, W = W)
grf::average_treatment_effect(fit_grf, target.sample = "overlap")
library(dplyr)
library(tmle)
# --- 0) 準備 ---------------------------------------------------------------
library(MatchIt)
data("lalonde")
dat <- lalonde |>
tidyr::drop_na()
# 処置・アウトカム・共変量
A <- dat$treat
Y <- dat$re78
W <- dat[, c("age","educ","race","married","nodegree","re74","re75")]
# 欠損があれば落とす（lalonde は通常欠損なし想定）
n <- length(Y)
# --- 1) まずは素朴な推定（比較用） ----------------------------------------
naive_diff <- mean(Y[A==1]) - mean(Y[A==0])
# --- 2) gモデル（傾向スコア）: P(A=1|W) -----------------------------------
g_fit <- glm(treat ~ age + educ + race + married + nodegree + re74 + re75, data = dat, family = binomial())
g_hat <- predict(g_fit, type = "response")
# 実務では positivity の極端さ対策で truncation をよく入れる
g_hat <- pmin(pmax(g_hat, 0.01), 0.99)
# --- 3) Qモデル（アウトカム回帰）: E(Y|A,W) ---------------------------------
Q_fit <- glm(re78 ~ ., data = dat, family = gaussian())
# 観測された A の下での予測 Q(A,W)
Q_aw <- predict(Q_fit, type = "response")
# 介入した世界（A=1, A=0）の下での予測 Q(1,W), Q(0,W)
Q1_w <- predict(Q_fit, newdata = dplyr::mutate(dat, treat = 1), type = "response")
Q0_w <- predict(Q_fit, newdata = dplyr::mutate(dat, treat = 0), type = "response")
# ここまでが「G-computation（回帰による反実仮想平均）」の素材
gcomp_ate <- mean(Q1_w - Q0_w)
# --- 4) clever covariate（IPWっぽい重み） -----------------------------------
# 連続アウトカムの基本形では、H1, H0 を使って Q を "必要な方向に" だけ更新する
H1 <- as.numeric(dat$treat == 1) / g_hat
H0 <- as.numeric(dat$treat == 0) / (1 - g_hat)
# --- 5) targeting step：epsilon を推定して Q を更新 --------------------------
# 連続(Y)の簡単な形： (Y - Q_aw) を H1, H0 で回帰（切片なし）して epsilon を得る
dat2 <- dplyr::mutate(dat,
Q_aw = Q_aw,
H1 = H1,
H0 = H0)
eps_fit <- lm(I(re78 - Q_aw) ~ -1 + H1 + H0, data = dat2)
eps <- coef(eps_fit)
eps1 <- unname(eps["H1"])
eps0 <- unname(eps["H0"])
# 更新後の Q*（observed と intervention の両方）
Qstar_aw <- Q_aw + eps1 * H1 + eps0 * H0
Qstar_1w <- Q1_w + eps1 * (1 / g_hat)
Qstar_0w <- Q0_w + eps0 * (1 / (1 - g_hat))
tmle_ate <- mean(Qstar_1w - Qstar_0w)
# --- 6) 標準誤差（influence function の分散で） ----------------------------
# IC = H1*(Y - Q*) - H0*(Y - Q*) + (Q*(1,W)-Q*(0,W)) - psi
IC <- (H1 - H0) * (Y - Qstar_aw) + (Qstar_1w - Qstar_0w) - tmle_ate
se <- sd(IC) / sqrt(n)
ci <- tmle_ate + c(-1, 1) * 1.96 * se
# --- 7) 結果表示 -----------------------------------------------------------
cat("n =", n, "\n")
cat("Naive diff (unadjusted):", naive_diff, "\n")
cat("G-computation ATE (Q-model only):", gcomp_ate, "\n")
cat("TMLE ATE (targeted):", tmle_ate, "\n")
cat("TMLE 95% CI:", ci[1], "to", ci[2], "\n")
library(tmle)
W <- model.matrix(~ age + educ + race + married + nodegree + re74 + re75 + 0, data = dplyr::select(dat, -re78, -treat))
Y <- dat$re78
A <- dat$treat
tmle_fit <- tmle::tmle(Y = Y, A = A, W = W,
family = 'gaussian',
Q.SL.library=c("SL.glm"),
g.SL.library=c("SL.glm"),
Qform = "Y~A+age+educ+raceblack+racehispan+racewhite+married+nodegree+re74+re75",
gform = "A~age+educ+raceblack+racewhite+racehispan+married+nodegree+re74+re75",
cvQinit = TRUE)
summary(tmle_fit)
tmle_fit$estimates$ATE$psi
tmle_fit$estimates$ATE$CI
tmle_fit$estimates$ATE$var.psi
library(dplyr)
library(AIPW)
library(easystats)
AIPW_SL <- AIPW$new(Y= lalonde_r$re78,
A= lalonde_r$treat,
W= subset(lalonde_r,select=c("age", "educ", "race", "married", "nodegree", "re74", "re75")),
Q.SL.library = c("SL.glm"),
g.SL.library = c("SL.glm"),
k_split = 5,
verbose=TRUE)
suppressWarnings({
AIPW_SL$fit()$summary(g.bound = 0.025)
})
suppressWarnings({
AIPW_SL$stratified_fit()$summary(g.bound = 0.025)
})
tmle_fit <- tmle::tmle(Y = Y, A = A, W = W,
family = 'gaussian',
Q.SL.library=c("SL.mean","SL.glm"),
g.SL.library=c("SL.mean","SL.glm"),
Qform = "Y~A+age+educ+raceblack+racehispan+racewhite+married+nodegree+re74+re75",
gform = "A~age+educ+raceblack+racewhite+racehispan+married+nodegree+re74+re75",
cvQinit = TRUE)
cat("\nEstimates from TMLE\n")
unlist(tmle_fit$estimates$ATE)
unlist(tmle_fit$estimates$RR)
unlist(tmle_fit$estimates$OR)
cat("\nEstimates from AIPW\n")
a_tmle <- AIPW_tmle$
new(
Y= lalonde_r$re78,
A= lalonde_r$treat,
tmle_fit = tmle_fit,
verbose = TRUE)$
summary(g.bound=0.025)
# For X learner
library(tidymodels)
library(dplyr)
set.seed(1)
# -----------------------
# Simulate data (binary treatment, continuous outcome)
# -----------------------
n <- 2000
p <- 6
X <- matrix(rnorm(n * p), n, p)
colnames(X) <- paste0("x", 1:p)
lin_p  <- 0.3 * X[,1] - 0.2 * X[,2] + 0.15 * X[,3]
e_true <- plogis(lin_p)
w      <- rbinom(n, 1, e_true)
mu0_true  <- 0.5 * X[,1] - 0.8 * X[,2] + 0.2 * X[,4]^2
tau_true  <- 1.0 + 0.7 * X[,1] - 0.4 * X[,2]
y         <- mu0_true + w * tau_true + rnorm(n, sd = 1.0)
dat <- as_tibble(X) |>
mutate(y = y, w = w, w_f = factor(w, levels = c(0, 1)), tau_true = tau_true)
xvars <- paste0("x", 1:p)
# -----------------------
# Train/Test split
# -----------------------
set.seed(2)
sp    <- initial_split(dat, prop = 0.7, strata = w_f)
train <- training(sp)
test  <- testing(sp)
# -----------------------
# Base learners
# -----------------------
outcome_spec <- rand_forest(trees = 500, mtry = 3, min_n = 10) |>
set_engine("ranger") |>
set_mode("regression")
tau_spec <- rand_forest(trees = 800, mtry = 3, min_n = 10) |>
set_engine("ranger") |>
set_mode("regression")
prop_spec <- logistic_reg(penalty = 0.001, mixture = 1) |>
set_engine("glmnet")
# -----------------------
# Step 1: nuisance outcome models mu0(x), mu1(x)
# -----------------------
mu0_fit <- workflow() |>
add_recipe(recipe(y ~ ., data = train |> filter(w == 0) |> select(all_of(xvars), y))) |>
add_model(outcome_spec) |>
fit(data = train |> filter(w == 0) |> select(all_of(xvars), y))
q()
library(MatchIt)
lalonde_r <- MatchIt::lalonde
library(parameters)
library(rms)
dd <- datadist(lalonde_r)
options(datadist = "dd")
fit_ols_rcs <- rms::ols(re78 ~ treat + rcs(age, 5) + educ + race + married + nodegree + rcs(re74, 5) + rcs(re75, 5), data = lalonde_r)
parameters::model_parameters(
fit_ols_rcs, keep = "treat"
)  |>
print_md()
library(dplyr)
library(grf)
X <- model.matrix(~ age + educ + race + married + nodegree + re74 + re75 + 0, data = dplyr::select(lalonde_r, -re78, -treat))
Y <- lalonde_r$re78
W <- lalonde_r$treat
fit_grf <- grf::causal_forest(X = X, Y = Y, W = W)
grf::average_treatment_effect(fit_grf, target.sample = "overlap")
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
